**Ð’Ñ‹Ð¿Ð¾Ð»Ð½Ð¸Ð»: Ð¡Ð¾Ð»Ð´Ð°Ñ‚ÐºÐ¸Ð½ ÐÐ»ÐµÐºÑÐ°Ð½Ð´Ñ€**
**Ð“Ñ€ÑƒÐ¿Ð¿Ð°: ÐŸÐ˜Ð-Ð±-Ð¾-22-1**

# Ð›Ð°Ð±Ð¾Ñ€Ð°Ñ‚Ð¾Ñ€Ð½Ð°Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ð° â„–7: Ð˜Ð½Ñ‚ÐµÐ³Ñ€Ð°Ñ†Ð¸Ñ Kafka Ð¸ Spark Streaming

1. Ð¦ÐµÐ»ÑŒ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹
ÐžÑÐ²Ð¾Ð¸Ñ‚ÑŒ Ð¸Ð½Ñ‚ÐµÐ³Ñ€Ð°Ñ†Ð¸ÑŽ Apache Kafka Ñ Apache Spark Streaming Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð²Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð² Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾Ð¼ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸. ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð¿Ñ€Ð°ÐºÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð½Ð°Ð²Ñ‹ÐºÐ¸ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ð¿Ñ€ÑÐ¼Ð¾Ð³Ð¾ Ñ‡Ñ‚ÐµÐ½Ð¸Ñ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸Ð· Ñ‚Ð¾Ð¿Ð¸ÐºÐ¾Ð² Kafka Ð¸ Ð¸Ñ… Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Structured Streaming.

2. Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼Ñ‹Ð¹ ÑÑ‚ÐµÐº Ñ‚ÐµÑ…Ð½Ð¾Ð»Ð¾Ð³Ð¸Ð¹

ÐŸÐ»Ð°Ñ‚Ñ„Ð¾Ñ€Ð¼Ñ‹: Apache Kafka 
Ð¯Ð·Ñ‹Ðº Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ: Python (PySpark)
Ð‘Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐºÐ¸: kafka-python, pyspark
ÐžÐ¡: Linux (Ubuntu) 
Ð˜Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ñ‹: Jupyter Notebook, Kafka CLI tools


3. Ð¢ÐµÐ¾Ñ€ÐµÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ ÑÐ²ÐµÐ´ÐµÐ½Ð¸Ñ

**Ð˜Ð½Ñ‚ÐµÐ³Ñ€Ð°Ñ†Ð¸Ñ Kafka-Spark** Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ Ð¾Ð±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°Ñ‚ÑŒ Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð²Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð½ÐµÐ¿Ð¾ÑÑ€ÐµÐ´ÑÑ‚Ð²ÐµÐ½Ð½Ð¾ Ð¸Ð· Ñ‚Ð¾Ð¿Ð¸ÐºÐ¾Ð² Kafka Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ Ð¼Ð¾Ñ‰Ð½Ð¾ÑÑ‚ÐµÐ¹ Spark Streaming.

**ÐšÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ðµ ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ñ‹ Ð¸Ð½Ñ‚ÐµÐ³Ñ€Ð°Ñ†Ð¸Ð¸:**
*   **Kafka Direct API:** ÐŸÑ€ÑÐ¼Ð¾Ðµ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ Ðº Ð¿Ð°Ñ€Ñ‚Ð¸Ñ†Ð¸ÑÐ¼ Ñ‚Ð¾Ð¿Ð¸ÐºÐ°
*   **Offset management:** Ð£Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¿Ð¾Ð·Ð¸Ñ†Ð¸ÐµÐ¹ Ñ‡Ñ‚ÐµÐ½Ð¸Ñ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹
*   **Exactly-once semantics:** Ð“Ð°Ñ€Ð°Ð½Ñ‚Ð¸Ñ Ð¾Ð´Ð½Ð¾ÐºÑ€Ð°Ñ‚Ð½Ð¾Ð¹ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹
*   **Structured Streaming integration:** ÐÐ°Ñ‚Ð¸Ð²Ð½Ð°Ñ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ° Ð² Spark Structured Streaming

**ÐŸÑ€ÐµÐ¸Ð¼ÑƒÑ‰ÐµÑÑ‚Ð²Ð° Ð¿Ð¾Ð´Ñ…Ð¾Ð´Ð°:**
*   Ð’Ñ‹ÑÐ¾ÐºÐ°Ñ Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ½Ð°Ñ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚ÑŒ
*   ÐžÑ‚ÐºÐ°Ð·Ð¾ÑƒÑÑ‚Ð¾Ð¹Ñ‡Ð¸Ð²Ð¾ÑÑ‚ÑŒ
*   ÐœÐ°ÑÑˆÑ‚Ð°Ð±Ð¸Ñ€ÑƒÐµÐ¼Ð¾ÑÑ‚ÑŒ
*   ÐŸÐ¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ° Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ñ… Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¾Ð² Ð´Ð°Ð½Ð½Ñ‹Ñ…
4. **Ð¥Ð¾Ð´ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹**

### Ð§Ð°ÑÑ‚ÑŒ 1: ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° Kafka-Ð¸Ð½Ñ„Ñ€Ð°ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñ‹

1.1 Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ñ‚Ð¾Ð¿Ð¸ÐºÐ° Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸

docker exec kafka kafka-topics.sh --create \
    --topic real-time-transactions \
    --bootstrap-server localhost:9092 \
    --partitions 3 \
    --replication-factor 1 \
    --config retention.ms=86400000  # Ð¥Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ 24 Ñ‡Ð°ÑÐ°

echo " Ð¢Ð¾Ð¿Ð¸Ðº 'real-time-transactions' ÑÐ¾Ð·Ð´Ð°Ð½"

docker exec kafka kafka-topics.sh --create \
    --topic transaction-alerts \
    --bootstrap-server localhost:9092 \
    --partitions 2 \
    --replication-factor 1

docker exec kafka kafka-topics.sh --create \
    --topic user-statistics \
    --bootstrap-server localhost:9092 \
    --partitions 2 \
    --replication-factor 1

echo " Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ñ‚Ð¾Ð¿Ð¸ÐºÐ¸ ÑÐ¾Ð·Ð´Ð°Ð½Ñ‹"

echo " Ð¡Ð¿Ð¸ÑÐ¾Ðº Ñ‚Ð¾Ð¿Ð¸ÐºÐ¾Ð²:"
docker exec kafka kafka-topics.sh --list --bootstrap-server localhost:9092

1.2 Producer Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ñ‚ÐµÑÑ‚Ð¾Ð²Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…

import json
import time
import random
from datetime import datetime
from kafka import KafkaProducer
import logging


logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TransactionProducer:
    def __init__(self, bootstrap_servers='localhost:9092'):
        """Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Kafka Producer Ð´Ð»Ñ Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸Ð¹"""
        self.producer = KafkaProducer(
            bootstrap_servers=bootstrap_servers,
            value_serializer=lambda v: json.dumps(v).encode('utf-8'),
            key_serializer=lambda v: str(v).encode('utf-8') if v else None,
            acks='all',
            retries=3,
            compression_type='gzip',
            linger_ms=10,
            batch_size=16384
        )
        logger.info(f" TransactionProducer Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½ Ðº {bootstrap_servers}")
    
    def generate_transaction(self, transaction_id):
        """Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ñ‚ÐµÑÑ‚Ð¾Ð²Ð¾Ð¹ Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸Ð¸"""
        transaction_types = ['purchase', 'refund', 'transfer', 'withdrawal', 'deposit']
        
        # Ð’ÐµÑÐ¾Ð²Ð°Ñ Ð²ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚ÑŒ Ð´Ð»Ñ Ñ‚Ð¸Ð¿Ð¾Ð² Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸Ð¹
        weights = [0.6, 0.1, 0.15, 0.1, 0.05]  # purchase, refund, transfer, withdrawal, deposit
        transaction_type = random.choices(transaction_types, weights=weights)[0]
        
        # Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ ÑÑƒÐ¼Ð¼Ñ‹ Ð² Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚Ð¸ Ð¾Ñ‚ Ñ‚Ð¸Ð¿Ð°
        if transaction_type == 'purchase':
            amount_range = (10.0, 500.0)
        elif transaction_type == 'refund':
            amount_range = (5.0, 200.0)
        elif transaction_type == 'transfer':
            amount_range = (50.0, 1000.0)
        elif transaction_type == 'withdrawal':
            amount_range = (20.0, 300.0)
        else:  # deposit
            amount_range = (100.0, 2000.0)
        
        transaction = {
            'transaction_id': f'txn_{transaction_id:06d}',
            'user_id': f'user_{random.randint(1, 50)}',  # 50 ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ… Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¹
            'amount': round(random.uniform(*amount_range), 2),
            'type': transaction_type,
            'timestamp': datetime.now().isoformat(),
            'location': random.choice(['online', 'store', 'atm', 'mobile_app']),
            'currency': random.choice(['USD', 'EUR', 'GBP']),
            'merchant_id': f'merchant_{random.randint(1, 20)}' if transaction_type in ['purchase', 'refund'] else None,
            'status': random.choices(['success', 'failed', 'pending'], weights=[0.85, 0.1, 0.05])[0],
            'category': random.choice(['electronics', 'food', 'clothing', 'entertainment', 'utilities', 'travel']) 
                        if transaction_type == 'purchase' else None,
            'fraud_score': round(random.uniform(0.0, 1.0), 3)  # Ð¡Ð¸Ð¼ÑƒÐ»ÑÑ†Ð¸Ñ ÑÐºÐ¾Ñ€Ð° Ð¼Ð¾ÑˆÐµÐ½Ð½Ð¸Ñ‡ÐµÑÑ‚Ð²Ð°
        }
        
        return transaction
    
    def send_transactions(self, num_transactions=1000, delay=0.5):
        """ÐžÑ‚Ð¿Ñ€Ð°Ð²ÐºÐ° Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸Ð¹ Ð² Kafka"""
        logger.info(f" ÐžÑ‚Ð¿Ñ€Ð°Ð²ÐºÐ° {num_transactions} Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸Ð¹...")
        
        metrics = {
            'sent': 0,
            'failed': 0,
            'by_type': {},
            'by_user': {}
        }
        
        for i in range(num_transactions):
            try:
                transaction = self.generate_transaction(i)
                user_id = transaction['user_id']
                
                # ÐžÑ‚Ð¿Ñ€Ð°Ð²ÐºÐ° Ñ ÐºÐ»ÑŽÑ‡Ð¾Ð¼ Ð´Ð»Ñ Ð¿Ð°Ñ€Ñ‚Ð¸Ñ†Ð¸Ð¾Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¿Ð¾ user_id
                future = self.producer.send(
                    topic='real-time-transactions',
                    key=user_id,
                    value=transaction
                )
                
                # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸
                metrics['sent'] += 1
                metrics['by_type'][transaction['type']] = metrics['by_type'].get(transaction['type'], 0) + 1
                metrics['by_user'][user_id] = metrics['by_user'].get(user_id, 0) + 1
                
                # ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ ÐºÐ°Ð¶Ð´Ñ‹Ðµ 50 Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸Ð¹
                if (i + 1) % 50 == 0:
                    logger.info(f"ðŸ“¤ ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¾ {i + 1}/{num_transactions} Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸Ð¹")
                
                time.sleep(delay)
                
            except Exception as e:
                metrics['failed'] += 1
                logger.error(f" ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¾Ñ‚Ð¿Ñ€Ð°Ð²ÐºÐµ Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸Ð¸ {i}: {e}")
        
        self.producer.flush()
        
        # Ð’Ñ‹Ð²Ð¾Ð´ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¸
        logger.info(f" ÐžÑ‚Ð¿Ñ€Ð°Ð²ÐºÐ° Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð°!")
        logger.info(f" Ð£ÑÐ¿ÐµÑˆÐ½Ð¾: {metrics['sent']}")
        logger.info(f" ÐžÑˆÐ¸Ð±Ð¾Ðº: {metrics['failed']}")
        
        logger.info(f" Ð Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ Ð¿Ð¾ Ñ‚Ð¸Ð¿Ð°Ð¼:")
        for ttype, count in metrics['by_type'].items():
            logger.info(f" {ttype}: {count}")
        
        return metrics
    
    def close(self):
        """Ð—Ð°ÐºÑ€Ñ‹Ñ‚Ð¸Ðµ producer"""
        self.producer.close()
        logger.info(" TransactionProducer Ð·Ð°ÐºÑ€Ñ‹Ñ‚")


producer = TransactionProducer()

try:
    metrics = producer.send_transactions(
        num_transactions=200,  # Ð£Ð¼ÐµÐ½ÑŒÑˆÐµÐ½Ð¾ Ð´Ð»Ñ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸Ð¸
        delay=0.3
    )
    
    print(f"   Ð˜Ð¢ÐžÐ“ÐžÐ’ÐÐ¯ Ð¡Ð¢ÐÐ¢Ð˜Ð¡Ð¢Ð˜ÐšÐ:")
    print(f"   Ð’ÑÐµÐ³Ð¾ Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸Ð¹: {metrics['sent'] + metrics['failed']}")
    print(f"   Ð£ÑÐ¿ÐµÑˆÐ½Ð¾ Ð¾Ñ‚Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¾: {metrics['sent']}")
    print(f"   ÐžÑˆÐ¸Ð±Ð¾Ðº Ð¾Ñ‚Ð¿Ñ€Ð°Ð²ÐºÐ¸: {metrics['failed']}")
    
except KeyboardInterrupt:
    print("\n\n ÐŸÑ€ÐµÑ€Ð²Ð°Ð½Ð¾ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¼")
except Exception as e:
    print(f"\n ÐšÑ€Ð¸Ñ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ°: {e}")
finally:
    producer.close()

### Ð§Ð°ÑÑ‚ÑŒ 2: ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Spark Streaming Ð´Ð»Ñ Ñ‡Ñ‚ÐµÐ½Ð¸Ñ Ð¸Ð· Kafka

2.1 Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Spark Session Ñ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ¾Ð¹ Kafka

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql.window import Window
import warnings
warnings.filterwarnings('ignore')

spark = SparkSession.builder \
    .appName("KafkaSparkIntegration") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0") \
    .config("spark.sql.streaming.checkpointLocation", "/tmp/spark-checkpoints") \
    .config("spark.sql.shuffle.partitions", "4") \
    .config("spark.streaming.backpressure.enabled", "true") \
    .config("spark.streaming.kafka.maxRatePerPartition", "1000") \
    .master("local[*]") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")

print(" Spark Session ÑÐ¾Ð·Ð´Ð°Ð½Ð° Ñ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ¾Ð¹ Kafka")
print(f" Ð’ÐµÑ€ÑÐ¸Ñ Spark: {spark.version}")
print(f" ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ:")
print(f"   - Checkpoint location: /tmp/spark-checkpoints")
print(f"   - Shuffle partitions: 4")
print(f"   - Backpressure enabled: true")

2.2 ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ ÑÑ…ÐµÐ¼Ñ‹ Ð´Ð»Ñ Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸Ð¹

transaction_schema = StructType([
    StructField("transaction_id", StringType(), True),
    StructField("user_id", StringType(), True),
    StructField("amount", DoubleType(), True),
    StructField("type", StringType(), True),
    StructField("timestamp", TimestampType(), True),
    StructField("location", StringType(), True),
    StructField("currency", StringType(), True),
    StructField("merchant_id", StringType(), True),
    StructField("status", StringType(), True),
    StructField("category", StringType(), True),
    StructField("fraud_score", DoubleType(), True)
])

print(" Ð¡Ñ…ÐµÐ¼Ð° Ñ‚Ñ€Ð°Ð½Ð·Ð°ÐºÑ†Ð¸Ð¹ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð°:")
transaction_schema

2.3 Ð§Ñ‚ÐµÐ½Ð¸Ðµ Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð²Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸Ð· Kafka

kafka_df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "real-time-transactions") \
    .option("startingOffsets", "earliest") \
    .option("maxOffsetsPerTrigger", "100") \
    .option("failOnDataLoss", "false") \
    .load()

print(" ÐŸÐ¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ Ðº Kafka ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¾")
print(f" Ð¡Ñ…ÐµÐ¼Ð° Kafka DataFrame:")
kafka_df.printSchema()

print(f"\n ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Kafka consumer:")
print(f"   - Bootstrap servers: localhost:9092")
print(f"   - Topic: real-time-transactions")
print(f"   - Starting offsets: earliest")
print(f"   - Max offsets per trigger: 100")
print(f"   - Is streaming: {kafka_df.isStreaming}")

2.4 ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ Ð¸ Ð¿Ñ€ÐµÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ…

def parse_kafka_data(df):
    # ÐŸÐ°Ñ€ÑÐ¸Ð¼ JSON Ð¸Ð· value ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸
    parsed_df = df.select(
        col("key").cast("string").alias("user_key"),
        from_json(col("value").cast("string"), transaction_schema).alias("data"),
        col("timestamp").alias("kafka_timestamp"),
        col("partition"),
        col("offset")
    ).select(
        "user_key",
        "data.*",
        "kafka_timestamp",
        "partition",
        "offset"
    )
    
    return parsed_df

parsed_transactions = parse_kafka_data(kafka_df)

print(" Ð”Ð°Ð½Ð½Ñ‹Ðµ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ñ€Ð°ÑÐ¿Ð°Ñ€ÑÐµÐ½Ñ‹")
print(f" Ð¡Ñ…ÐµÐ¼Ð° Ñ€Ð°ÑÐ¿Ð°Ñ€ÑÐµÐ½Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…:")
parsed_transactions.printSchema()

2.5 Ð‘Ð°Ð·Ð¾Ð²Ð°Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…

processed_df = parsed_transactions \
    .filter(col("status") == "success") \
    .withColumn("processing_timestamp", current_timestamp()) \
    .withColumn("amount_usd", 
                when(col("currency") == "EUR", col("amount") * 1.1)
                .when(col("currency") == "GBP", col("amount") * 1.3)
                .otherwise(col("amount"))
               ) \
    .withColumn("is_high_value", col("amount") > 500.0) \
    .withColumn("is_suspicious", col("fraud_score") > 0.8) \
    .select(
        "transaction_id",
        "user_id",
        "type",
        "amount",
        "currency",
        "amount_usd",
        "location",
        "category",
        "status",
        "fraud_score",
        "is_high_value",
        "is_suspicious",
        "timestamp",
        "kafka_timestamp",
        "processing_timestamp",
        "partition",
        "offset"
    )

print(" Ð”Ð°Ð½Ð½Ñ‹Ðµ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ñ‹")
print(f" Ð¡Ñ…ÐµÐ¼Ð° Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…:")
processed_df.printSchema()